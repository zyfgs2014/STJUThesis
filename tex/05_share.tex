%# -*- coding: utf-8-unix -*-

\chapter{结合共享账户的机票推荐算法}
\label{chap:share}

本章我们研究机票推荐中的共享账户问题，该问题主要针对一个用户ID包括多个乘客的情况。我们将提出一种算法，可以预测出用户下本次购票的乘客的概率分布，从而学习更具针对性的偏好模型，提升推荐准确率。

\section{机票推荐中的共享账户}

推荐算法总体分为两类，一类是根据用户和物品之间的关系，提出衡量用户相似度以及物品相似度的算法，为用户根据物品或用户之间的相似度提供推荐结果；另一类是基于内容的推荐，物品可以表示为特征的集合（包括显性特征和隐性特征），从用户的物品列表中学习用户对每个特征的偏好。为用户推荐最符合其特征偏好的物品。

以这两类算法为代表的传统推荐系统通常以用户在网站的注册ID（账户）来识别用户，它们一般假定一个用户背后只有一个使用者，即每个用户仅包含一个固定的偏好模型。然而在很多领域，都会发生现实中几个人共同使用一个账户的情景。例如，在购物网站，可能一个家庭共用一个账户，每位家庭成员都可以使用这个账户进行购物；在一般的线旅行票务服务网站，例如机票、酒店、旅游等业务领域，一个账户中也可能会有几个成员，他们可能会为家人、旅伴购票。


第一种情形中，虽然会有几位家庭成员共同使用账户购物，但是服务商无法获取购物者的真实身份信息。而对于第二种情形，如图\ref{fig:fill_id}所示，乘客在订票后会被要求填写身份信息，因此，网站可以获取其真实身份。在机票数据集中，订单包含了每一位乘客的身份信息，包括姓名、性别、年龄、证件号码等。在我们进行数据分析与实验过程中，统一使用加密后的证件号码来代指乘客，既确保了乘客之间无冲突，又保护了乘客隐私。

\begin{figure}
 \centering
 \includegraphics[width=0.5\linewidth]{05/1_fill_id.png}
 \bicaption[fig:fill_id]{填写乘客信息}{用户在票务网站填写身份信息}{Fig}{Users Fill in Their Identification on Website}
\end{figure}

通常来说，同一个用户下乘客可能具有较为相似的偏好，但他们之间的差异也不能忽视。如果我们能够利用购票时获取的乘客身份信息为该乘客进行机票推荐，就可以为每位乘客建立特征分布模型，更具针对性的细粒度乘客模型对进一步提升个性化机票推荐准确率可以起到作用。然而，按照业务流程，只有当用户选定机票后才会填写身份信息。在我们进行机票推荐时还无法获取乘客信息。我们需要预测乘客的概率分布。

我们的问题是在共享账户的乘客选购机票时，根据乘客历史行为模式以及当前会话的上下文预测出本次购票的目标乘客。在电子购物网站等无法获得成员真实身份的场景中，为了解决共享账户问题，通常需要设置几个虚拟成员，其中每个成员代表一种偏好模型。虚拟成员的数量可以是固定值，也可以是通过模型计算出的最优值。然而，这类算法通常只分析用户和物品之间的关系，无法利用当前会话中的上下文信息；并且对虚拟成员预测的分布准确率也无法验证。在机票推荐场景中，对乘客的预测准确率是可以进行验证的。

在机票的乘客预测模型中，我们使用了作者主题模型（ATM）来分析乘客的行为模型。主题模型是一种生成模型，其主要观点是一篇文档可以抽取出混合的多个主题。作者主题模型在原主题的基础上又添加了作者的关系。每个账户独立进行模型训练，可以将每个账户看作一个语料库。账户中出现过的所有乘客看作每位作者，每条机票订单看作一篇文档， 机票中的每个特征以及上下文信息看作预定义的词库。如果出现一张机票有多个乘客的情况，我们将他们视为共同作者。

\section{乘客预测模型}
事实上，主题模型在推荐系统中的应用很广泛。例如，在pLSA模型中，每个物品可以视作一个词汇，主题服从对物品的多项式分布，每个主题代表了一种隐性特征。每用户的偏好模型都服从对主题的多项式分布。用户的每次购买行为都可以被看作从该用户中抽样一个主题$Z$，再从该主题中抽取一个词汇的过程。由于机票具有动态属性，很难将不同价格的机票定义为同一个物品；并且每类机票的数量受限于飞机的型号，因此上述pLSA模型很难用到机票推荐领域中。

\subsection{模型描述及表示}

在作者主题模型中，我们用机票在每个特征离散化后的取值来预定义一个词库。这个词库包含了机票在任一特征中所有可能出现的值，使用从0开始的索引来代指每一个词汇。至此，每一张机票都可以表示为词汇的集合，我们将每条订单视作一篇文档，显然，所有订单文档中的的词汇数量都是相同的。每个主题都由词汇的多项式分布表示；类似地，每个乘客由主题的多项式分布表示。因此
ATM模型可以对文档（订单）进行降维。通过模型训练，可以计算出乘客与主题之间的分布参数以及主题与词汇之间的分布参数。表\ref{tab:notation}描述了我们本章使用的符号表。

\begin{table}[!hpb]
\centering
  \bicaption[tab:notation]{符号表}{乘客预测模型符号表}{Table}{Notation for ATM}
\begin{tabular}{|c|c|} \hline
$M$ & 数据集账户数量\\ \hline
$V$ & 词库中单词的数量\\ \hline
$O$ & 一个账户中所有订单\\ \hline
$P$ & 一个账户中所有乘客\\ \hline
$P_i$ & 订单$O_i$对应的乘客集合 \\ \hline
$F$ & 机票特征集合\\ \hline
$K$ & 主题数量\\ \hline
\end{tabular}
\end{table}

对于一个账户$M$，我们获取其中包含的乘客集合$P$。我们使用矩阵$\Theta$来表示每位乘客的主题分布，这个参数矩阵的维度是$|P| \times K$，$K$代表模型主题的数量。$\Theta$以狄利克雷分布作为先验，该先验的超参数是$\alpha$。矩阵$\Phi$可以表示每个主题在词汇上的分布，该矩阵的维度是$V \times K$。$\Phi$同样以超参数为超参数是$\beta$的狄利克雷分布作为先验。通常情况下，超参数不是通过训练得到的，而是根据多次数据实验的经验总结来赋值。本章中，我们将$\alpha$赋值为$50/K$，将$\beta$赋值为0.01。

式\ref{eq:dir}描述了狄利克雷分布。其中，带$\Gamma$函数的那一项是常系数。可以发现，狄利克雷分布与多项式分布具有相同的形式。这两个分布是共轭分布。

\begin{eqnarray}
\label{eq:dir}
	Dir(\mathbf{p}|\alpha) & = & \frac{1}{B(\alpha)}\prod_{i=1}^n p_i^{\alpha_i-1} \nonumber \\
	& = & \frac{\Gamma(\sum_{i=1}^n \alpha_i)}{\prod_{i=1}^n \Gamma(\alpha_i)}\prod_{i=1}^n p_i^{\alpha_i-1}
\end{eqnarray}

在训练数据中，一条机票订单的乘客集合是明确的，每张机票的特征内容，即该文档的所有词汇也是明确的；为了训练两个矩阵参数，我们需要为订单中的每个词汇赋予乘客与主题。这两个变量属于隐性变量。首先我们从这条订单的乘客列表中按均匀分布抽取出一位乘客；然后根据乘客-主题参数矩阵抽取出一个主题$Z$，再根据主题-词汇参数矩阵抽取一个单词$w$，我们使用下列流程对生成模型进行数学描述：

\begin{enumerate}
\item 对每个账户中的所有乘客$p$，以狄利克雷先验初始化 $\Theta_p \sim Dir(\alpha)$
\item 对每个主题$t$，以狄利克雷先验初始化 $\Phi_t \sim Dirichlet(\beta)$
\item 对账户下的每条订单$o$
       \begin{enumerate}[fullwidth,itemindent=1em,label=(\alph*)]
       \item $P$ 表示这条订单的乘客集合
       \item 对于订单中的每一个词汇
              \begin{enumerate}[fullwidth,itemindent=2em,label=(\roman*)]
              \item 按均匀分布从$P$中抽取一位乘客 $X_{oi} \sim Uniform(P)$
              \item 从乘客-主题矩阵中抽取一个主题 $Z_{oi} \sim Discrete(\theta_{X_{oi}})$
              \item 从主题-词汇矩阵中抽取一个单词 $w_{oi} \sim Discrete(\phi_{Z_{oi}})$
              \end{enumerate}
       \end{enumerate}
\end{enumerate}

图\ref{fig:pro_graph}展示了作者主题模型的概率图。带阴影的变量是已知变量；其余是未知变量。每个方框代表一个过程，其右下角的数字是这个过程的循环次数。箭头连接线代表了变量间的条件依赖关系。这个图直观、清晰地表现了生成一条订单文档的过程。

\begin{figure}
 \centering
 \includegraphics[width=0.45\linewidth]{05/2_graph.eps}
 \bicaption[fig:pro_graph]{模型概率图}{作者主题模型的概率图}{Fig}{Probability Graph for the Author-topic Model}
\end{figure}

\subsection{模型参数推导}
在上节的模型描述中，我们介绍了作者主题模型的两个参数矩阵，分别是包含$P$位乘客的乘客-主题分布参数以及包含$K$个主题的主题-词汇分布。有一些算法可以用来推导这些参数，如EM算法、Gibbs采样等。传统的EM算法可能会陷入局部最优问题，并且这种算法的计算复杂性较高。本章，我们使用Gibbs采样的算法，这种算法不直接进行参数推导，而通过计算抽取的乘客和主题的后验分布，对参数进行更新。因此计算过程较为简单。


\begin{eqnarray}
\label{eq:all_pro}
P(\mathbf{w}_o | \Theta,\Phi,P) & = & \prod_{i=1}^{N_o}P(w_{oi}|\Theta,\Phi,\mathbf{p}_o) 
\nonumber \\
 & = & \prod_{i=1}^{N_o}\sum_{p=1}^{|P|}\sum_{t=1}^{K}P(w_{oi}|z_{oi}=t,\Phi)
P(z_{oi}=t|x_{oi}=p,\Theta)P(x_{oi}=p|\mathbf{p}_o) \nonumber \\
 & = & \prod_{i=1}^{N_o}\frac{1}{|P|}\sum_{p \in p_o}\sum_{t=1}^{K}\phi_{w_{oi}t}\theta_{tp}
\end{eqnarray}


式\ref{eq:all_pro}描述了在参数矩阵$\Theta$，$\Phi$和乘客集合$P$的条件下，生成一条机票订单的概率函数。在这个生成模型中，$P(w_{oi}|z_{oi}=t,\Phi)$是在选定主题的条件下，根据主题-词汇分布矩阵抽取单词的概率。
$P(z_{oi}=t|x_{oi}=p,\Theta)$是在选定乘客的条件下，根据乘客-主题分布矩阵抽取主题的概率。$P(x_{oi}=p|\mathbf{p}_o)$是乘客集合中以服从均匀分布抽取乘客的概率。这个等式可以看作生成一条订单的似然函数。如果将参数$\Theta$，$\Phi$视为随机变量，我们训练模型的目标就是训练变量，使概率函数具有最大后验分布(MAP, Maximum A Posteriori)。 

在Gibbs采样的过程中，为了从联合分布$P(\mathbf{z},\mathbf{x}|\alpha,\beta)$中取样，我们需要为每一个单词$w_{di}$赋值一个主题$z_{di}$以及乘客$x_{di}$。在训练过程中，一个账户下的所有单词都会被取样一次。当所有单词都训练过，称为一个训练批次。通常，我们需要将模型训练数个批次。$p(\Theta,\Phi|\mathbf{z},\mathbf{x},\alpha,\beta)$可以根据狄利克雷和多项式分布四共轭分布的性质来计算。为每个单词抽取的乘客、主题可以使用下式进行计算：

\begin{eqnarray}
\label{eq:pro_pass_top}
P(x_{oi}=p,z_{oi}=t|w_{oi}=w,\mathbf{z}_{-oi},\mathbf{x}_{-oi},\mathbf{w}_{-oi},\alpha,\beta,p_o) \nonumber\\
\propto \frac{C_{tp}^{TP}+\alpha}{\sum{t'}C_{t'p}^{TP}+T\alpha}\frac{C_{wt}^{WT}+\beta}{\sum_{w'}C_{w't}^{WT}+W\beta}
\end{eqnarray}

式\ref{eq:pro_pass_top}代表为订单$o$的第i个单词赋予乘客p和主题t的概率。$C^{WT}$ 是单词-主题分布矩阵，其每行对应一个单词，每列对应一个主题，每个值代表当前单词被赋予当前主题的次数。$C_{wt}$是除了当前单词之外，单词$w$被赋予主题$t$的次数。$C^{TP}$是主题-乘客分布矩阵，其每行对应一个主题，每列对应一个乘客，矩阵中每个值代表对与该乘客在当前主题包含的单词个数。$C_{tp}$代表乘客$p$在主题$t$包含的词汇个数，同样排除了当前单词。我们可以根据抽样的过程对主题-词汇分布参数以及乘客-主题分布参数进行更新：

\begin{equation}
\label{eq:es_the}
\theta_{tp} = \frac{C_{tp}^{TP}+\alpha}{\sum{t'}C_{t'p}^{TP}+T\alpha}
\end{equation}

\begin{equation}
\label{eq:es_phi}
\phi_{wt} = \frac{C_{wt}^{WT}+\beta}{\sum_{w'}C_{w't}^{WT}+W\beta}
\end{equation}

式\ref{eq:es_the}和式\ref{eq:es_phi}分别两个参数的更新公式。$\theta_{tp}$是主题t在乘客p上的概率分布；$\phi_{wt}$是单词w在主题t上的概率分布。在每次取样后，
需要为每条订单更新单词-主题列表$T$以及单词-乘客$P$列表，其中$T[o][i]$代表在订单$o$中第$i$个单词的主题；$P[o][i]$代表在订单$o$中第$i$个单词的作者。因此，在参数推导的过程中，
我们需存储上文提到的$C^{TP}$和$C^{WT}$两个矩阵，以及每条订单中每个单词的主题和乘客两个列表。


模型训练主要包括三个步骤，分别是初始化，取样以及参数更新。在初始化过程中，我们为语料库中每条订单文档的每个单词随机分配主题和乘客。然后我们需要统计词汇表中每个单词被赋予某个主题的次数以及每个乘客在每个主题下的词汇的个数。在每次取样过程中，我们根据式\label{eq:p_xz}为语料库中的每个单词计算每个主题以及乘客的联合概率分布。我们根据概率为该单词采样新的主题和乘客。在几个批次的迭代训练后，可以根据式式\ref{eq:es_the}和\ref{eq:es_phi}对参数矩阵$\Theta$，$\Phi$进行更新。

在最差情况下，模型每批迭代的时间复杂度是$O(NK|P|)$。其中$N$是一个账户下所有订单的总词汇数量。$|P|$是该账户下乘客最多的订单中乘客的数量，一般不会很大，与订单的总词汇数量相比可以视作常量。$K$是模型的主题数量。主题数量的取值不是固定的，我们可以在实验中确定最适合机票推荐业务中，乘客预测模型的数量。在模型迭代过程中，$|P|$以及$K$都可以看作常量，因此，使用Gibbs采样训练ATM模型的时间复杂度可以看作与总订单词汇数量$N$呈线性关系，可以用在大量用户数据的训练任务中。

\begin{algorithm}
\caption{ATM模型训练}
\label{algo:atm_train}
\begin{algorithmic}[1]
\Require
\Statex 账户历史订单转化词汇表 $W$

\Ensure 
\Statex 乘客-主题分布参数 $\Theta$
\Statex 主题-词汇分布参数 $\Phi$

\State $nak , nkv, nak\_sum, nkv\_sum \gets 0$
\State $pl , tl \gets 0$
\For{$o \in W$}
\For{$w \in o$}
\State Pick a topic $t$ and a passenger $p$ uniformly;
\State passenger-topic count $nak[p][t] += 1$;
\State passenger-topic sum $nak\_sum[p] += 1$;
\State topic-word count $nkv[t][w] += 1$;
\State topic-word sum $nkv\_sum[t] += 1$;
\State word-topic list $tl[o][w] = t$
\State word-passenger list $pl[o][w] = p$
\EndFor
\EndFor
\While {not reach iteration limit}
\For{$o \in W$}
\For{$w \in o$}
\State $nak[p][t] -= 1, nak\_sum[p] -= 1$;
\State $nkv[t][w] -= 1, nkv\_sum[t] -= 1$;
\State Sample a new passenger $p'$ and topic $t'$ by Eq \ref{eq:pro_pass_top};
\State $nak[p'][t'] += 1, nak\_sum[p'] += 1$;
\State $nkv[t'][w] += 1, nkv\_sum[t'] += 1$;
\State $tl[o][w] = t', pl[o][w] = p'$;
\EndFor
\EndFor
\EndWhile
\State Calculate $\Theta,\Phi$ by Eq \ref{eq:es_the},\ref{eq:es_phi};
\State \Return $\Theta,\Phi$;
\end{algorithmic} 
\end{algorithm}

算法\ref{algo:atm_train}描述了ATM模型的训练过程。该过程的输入是账户下历史订单根据预定义词库转化的词汇表；输出是两个参数矩阵。其中第1至13行是模型初始化过程；其中第14至25行是使用Gibbs取样过程，每次取样将单词的乘客和主题进行更新，在计算过程中，我们增加了两个矩阵存储中间变量的结果；最后两行对参数进行更新。

\subsection{使用ATM模型进行乘客预测}

上一节我们介绍了在机票推荐领域中作者-主题模型的训练过程。我们首先定义一个词库，将账户下所有订单用作训练，并最终得到参数矩阵$\Theta$和$\Phi$。我们的目的是利用ATM模型，预测当前会话中账户下所有乘客购票概率分布。该问题在本质上属于分类问题，我们可以计算出每位乘客的购票概率，按概率将乘客进行排序。排位越靠前的乘客，越可能是本次购票者。

\begin{equation}
\label{eq:pred_pass}
p(x=p|o_n,\Theta,\Phi) \propto p(p)\prod_{w \in o_n}\sum_t p(t|w)p(w|t,p)
\end{equation}

式\ref{eq:pred_pass}的含义是在参数$\Theta$，$\Phi$的条件下，词汇集合$o_n$属于乘客$p$的的概率，相当于$p$属于订单$o$的乘客的概率。第一项的含义$p(p) = |O_p| / |O|$，$|O_p|$是训练集中乘客$p$参与的订单数量；$p(w|t,p)$是在给定主题和乘客的条件下，取样单词$w$的概率，由于从乘客中采样主题与从主题中采样单词的过程相互独立，因此$p(w|t,p) = p(t|p) \times p(w|t)$。$p(t|w)$代表单词$w$被赋予主题$t$的概率，可以通过迭代完成后的$C^{WT}$矩阵计算出来。

在乘客预测过程中的还有一个问题需要注意。我们在训练模型时可以使用订单的全部特征内容作为词汇列表；然而在机票推荐场景下，我们无法获得用户将要选购的机票的所有特征内容，因此不能用与训练数据一样的词汇表。因此，我们需要挖掘更多可能获取的特征，如起飞城市、到达城市、订票提前天数、登录网站的时间、IP地理位置等信息；以及用户行为上下文，包括搜索、点击、筛选等行为。因此我们在训练模型时，除了收集机票的特征，还需要将这些信息也转化为词汇，一并参与训练。

获取了每位乘客的概率分布后，为了针对乘客进行建立更细粒度的偏好模型，我们需要提供确切的参与该订票会话的乘客列表。平均每位乘客的概率是$\frac{1}{|P|}$，而我们只取概率超过平均概率的乘客。将这部分乘客视作参与本次订票的乘客集合。

至此，我们使用作者-主题模型对当前会话的乘客进行了预测，预测过程总体分为两个步骤。第一步，我们生成了一个预定义词库，使用Gibbs采样迭代训练了模型参数矩阵$\Theta$和$\Phi$；第二步，我们使用公式\ref{eq:pred_pass}为当前会话的乘客概率分布进行预测，并最终决定了参与本次订单的乘客集合。

\section{结合乘客预测的机票个性化推荐}
上一节介绍了机票推荐中的乘客预测算法，本节我们将乘客预测结合到个性化机票推荐中。旨在细化偏好模型的粒度，为乘客建立更有针对性的特征分布模型。进一步提升推荐准确率。

\begin{figure}
 \centering
 \includegraphics[width=0.6\linewidth]{05/3_overview.eps}
 \bicaption[fig:over_pr]{预测推荐概览}{结合乘客预测的机票推荐概览}{Fig}{Overview of Passenger Prediction based Recommendation}
\end{figure}

图\ref{fig:over_pr}是结合乘客预测的机票推荐流程概览图。我们的数据源是存储在数据仓库中的用户机票订单数据。一方面我们可以利用这部分数据进行作者-主题模型训练，将训练后的模型用于乘客预测；另一方面，这部分数据还用来构建表示用户偏好的特征分布模型，这里我们不根据账户ID为粒度进行建模，而是为账户中每一位乘机人建立模型。这两个部分都是可以进行离线计算的。此外我们还结合在用户购票会话中的上下文信息，进行乘客预测；并结合乘客预测和机票推荐的结果，对候选机票列表进行排序，得到最终推荐结果。

然而，每位用户的乘客预测模型并不是一成不变的。用户每购买一次机票，我们都需要为该账户更新ATM模型，将本次订单包括进去。最佳的更新策略是每添加一条新的订单时，都重新训练模型，但这会带来计算效率低下的问题。我们可以使用一种蒙特卡洛算法进行更新，这种更新策略只关注新订单中的词汇，因此我们可以快速为新订单中的词汇赋值乘客与主题。首先，我们将该条订单中的词汇随机赋予乘客与主题；然后我们对新订单中的词汇按照式\ref{eq:pro_pass_top}重新取样乘客和主题，并更新矩阵$C^{TP}$和$C^{WT}$，最终更新两个参数矩阵。由于新订单中词汇较少，只需要十次左右的迭代过程就可以完成训练。

获取到乘客的概率分布并选取出参与当前订单的乘客后，我们将这些乘客的特征分布模型读取出来，并进行融合。融合的策略可以按照计算出的乘客分布概率进行加权，我们除了考虑乘客分布概率外，还需要考虑乘客的订单总量，这里我们给出乘客特征分布模型的融合策略：
\begin{equation}
	\label{eq:dis_com}
	\mathbf{D'} = \sum_{p \in P'} p(x = p|o_n,\Theta,\Phi) * \log |O_p| * \mathbf{D_p}
\end{equation}

式\ref{eq:dis_com}给出了多乘客的特征分布融合公式。其中$P'$是本次会话预测的参与乘客集合。我们为每位乘客赋予的权重是该乘客的参与概率以及该乘客参与的订单数量取以10为底的对数。得到了融合的特征分布模型后，我们就可以将之用在机票推荐上。

\section{实验结果分析}

本节我们对前几节提出的基于作者-主题模型的乘客预测及机票推荐算法进行实验评估与分析。

\subsection{实验数据集与评价指标}


\subsection{乘客预测准确率}


\subsection{机票推荐准确率}

\section{本章小结}